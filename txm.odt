Notes des tutos TXM sur Youtube : 

Textométrie = lexicométrie = logométrie = statistique textuelle. Le texte prend son sens dans un corpus. La textométrie n’est pas là pour éviter le texte. La textométrie pointe des choses singulières qu’il faut interpréter. Il faut que les questions soient bonnes. Créer un corpus est déterminant, biaise la recherche quoi qu’il arrive. Il faut faire des choix (corpus, unités, choix des termes,…). Rien n’est donné, il faut tout construire. Pour comprendre les résultats, il faut comprendre les calculs. Il peut y avoir une multiplicité d’interprétations.. L’ordinateur n’est pas là pour nous remplacer, mais plutôt pour retenir des données et les traiter, les calculer. Le reste, c’est à nous de le faire. 
Il existe d’autres logiciels (lexico3, hyperbase, le tramer ?, Weblex, iramutech, alcest). Repose sur R pour les analyses statistiques. Analyse factorielle, TXM est pas mal mais y a mieux (important à faire). TXM est né dans le cadre d’un projet ANR, en 2007, pour fédérer les recherches en textométrie autour d’un logiciel libre. TXM reprend les corpus tels qu’ils sont, avec toutes leurs richesses (annotations,…). Points forts dans TXM : multiplateforme (Mac, Linux, Windows) et existe même sur le web. Souplesse de l’import (peut lire des données de toutes sortes). Les entrées vont du texte brut (TEI, Alcest,…) à d’autres formes plus complexes. On peut travailler aussi sur une grande variété de langues (tritagger).
On s’appuie sur CQP pour les recherches. TXM communique avec R et avec CQP, ils travaillent ensemble. Txm est libre et modulaire. 
Il y a deux commandes de base : importer et Charger. Importer c’est partir des textes où TXM fait tout à sa sauce. Il existe des corpus qui sont déjà indexés,… Les corpus déjà travaillés sont à charger. Pour les projets à porter depuis le début sont à importer. 
Clic droit, dans l’onglet « Propriétés », sont expliqués la séparation typologique du corpus, à savoir les mots, les lemmes, les pos (types de mots (verbes, noms, adverbes,…), vient de « part of speech ».  
Que dire sur le lexique ? On peut le trier par mots, lemmes. L’indication « t6175838 » correspond au nombre total de mots dans le corpus. Le « v223028 » correspond aux mots différents. C’est un premier aperçu du corpus, on peut faire des premières remarques. Des premières tendances peuvent apparaitre. On comprend facilement ce qu’on fait. Cela permet d’essayer de comprendre pourquoi tel mot se trouve si souvent dans le corpus. On peut aussi utiliser le lexique pour analyser exhaustivement le corpus. En règle générale, les apax concernent la moitié du corpus. On peut les négliger, on peut les analyser. En règle générale, la fréquence est proportionnelle au rang. Le 10e terme du lexique est approximativement 10 fois moins présent que le 1er terme, le 100e 100 fois moins,...
Les hapax sont souvent négligeables. Ils représentent la moitié d’un corpus. Le premier mot le plus fréquent est rarement indicatif. On peut comparer les formes graphiques et les lemmes en faisant glisser la fenêtre. On peut ainsi diviser la page en deux et regarder les points communs et différences.


